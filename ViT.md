**摘要部分**
介绍了transformer在NLP领域得到很深的应用，现在可以引入CV领域，并且之前引用了的结构，并没有完全改变原先的模型结构，比如CNN，实际上是可以完全脱离这些结构的

**引言部分**
首先先介绍了一下transformer模型，该模型最初应用于NLP模型，处理的逻辑是对于一个序列的输入，它会计算每个元素间的相似度，所谓的自注意力机制
那么如果要应用到CV里面的话，就是如何把图像一个2d的信息转化为1d的信息，单纯的拉伸是不行的，因为输入会太多，数量级过大
那么如何处理就是当今面对的问题，如何去降低序列强度就是一个方向，比如把特征图的特征提取当作是一个transformer的输入
还有孤自注意力，就是不把整张图作为输入，而是仅仅把一个局部大小可控的信息作为输入，这跟卷积的思想就有部分是相似的
对于轴自注意力机制，就是不要H*W,而是H+W，转化成两个1d的输入，这样就完全取代了原先的卷积操作
但是以上的两种方式都还是没有在现在的硬件上去加速，导致很难成为一个大模型
所以在大规模的图像识别上，还是传统的残差网络处理效果最佳

所以本篇论文还是想像对NLP中那样，让transformer也一样在CV有着更广泛的应用，直接应用transformer，而不对其本身的模型进行修改
处理方式就是把原先的图片分成一个一个的patch，符合transformer的序列长度输入，把这些patch的经过处理之后就可以看作一个一个单词，一般是把一个图片看作很多16*16的单词
在做好预训练的基础上，采用该方法得到的结果都是要优于传统的卷积神经网络
但是从结果上看比传统的卷积神经网络会差一点，这是由于其缺少像卷积神经网络的归纳偏置
其一是locality，就是相邻的区域其关联性是比较强的，其二是平移等变性，即先做平移还是先做卷积的结果都一样，具体来说，对于一个物体在图片中无论是在什么位置输出的结果都一样
有了这些先验信息，传统的卷积网络就可以从较少数据学到较好的模型
不过如果在够大的数据集上学习到足够多的信息，就可以在数据学习上取得比卷积神经网络更好的结果

这里也提到了引言的书写，先写做这个工作的原因，比如它的前景好，然后讲前面所做的相关工作，然后再是自己做的工作跟前人有什么区别，最后放结果

**结论部分**
拿NLP领域标准的transformer模型来做视觉的任务，跟之前的工作相比，除了在最开始提取patch的时候，没有用图像特有的归纳偏置

**相关工作部分**
讲到了transformer在NLP中的应用，这里有两个大模型，一个是BERT，一个是GPT，前者是通过完形填空的方式来预训练，拿掉中间的词，后者是通过预测下一个单词来进行预训练
这其实都是人为定的，只是人为地拿掉其中的一些词，所以这叫自监督的训练模式

相关工作部分就是要讲清跟你这篇论文相似工作的论文，然后再阐述清楚你们之间的区别是什么

**主体部分**
先介绍一下流程图的大概
首先是经过一个映射层，把每个patch进行处理，然后跟每个patch的位置编码进行一个结合，作为一个输入
接着把输入经过transformer编码器，得到一定的输出，这里还引入了一个特殊的embeding，因为每个token是相互学习的，所以用一个特定的token就可以学到所有的信息
中间的层即为全连接层，
比如原先的图像为224*224*3，分成16*16的patch之后，就是196*（16*16*3）即为196*768 ，而全连接层为768（固定，这是有前面patch的大小算来的）*768（根据一个transformer的具体大小而定）
这样就把图像转换成了196个token，每个token的大小为768,然后还有一个额外的token，维度也是768，这样方便后续的处理，所以在这里就是196+1个768的token
对于位置向量，是有一个表，长度也是768，所进行的操作就是跟原先的token进行相加，就得到了一个含有位置信息的token
然后经过L个transformer模块，然后把原来加入的特殊的cls，作为这个图像的特征输出
在拿到输出后，在后面接一个MLP通用分类头，最后用交叉熵函数去进行模型的训练
用tanh作为一个非线性的激活函数，去做分类的预测
至于为什么要用这个特殊的cls
作者也做了消融实验，也可以采用全局平均池化，得到一个全局的特征然后去做分类，其实效果都是差不多的，只是为了尽量去接近NLP任务中，防止说针对CV进行改进所带来的影响
那么混合模型的思路就是把原图像经过卷积神经网络处理，比如Resnet-50然后得到一个14*14的特征图，然后拉成1d的信息然后再放入transformer模块进行处理
52min








pre文稿
transformer模型最初应用于NLP领域，在NLP领域中取得了很好的效果，并成为了主流的模型，由于transformer的计算效率与可扩展性，使得可训练模型达到一个空前的规模
那么如果要应用到CV里面的话，问题就转化为如何把图像一个2d的信息转化为1d的信息，这样就可以作为transformer的输入了

不过单纯的拉伸是行不通的，因为输入会太多，数量级过大，transformer模型无法接收如此庞大的输入
那么如何处理就是当今面对的问题，如何去降低序列强度就是一个方向
比如把特征图的特征提取当作是一个transformer的输入
还有孤自注意力，就是不把整张图作为输入，而是仅仅把一个局部大小可控的信息作为输入
对于轴自注意力机制，就是不要H*W,而是H+W，沿两个轴，转化成两个1d的输入

不过上述的处理方法存在着一些局限
一是没有对图像信息充分的利用，比如轴自注意力机制就极端的取了两个轴上的信息
二是并没有完全脱离原先CV的处理方式，仍然依赖原先的CNN架构
而ViT模型自身的贡献是在于以下两个方面
其一是使用NLP领域标准的transformer模型来做视觉任务，这样可以让它的预训练规模扩大
其二是除了最开始提取patch的时候，没有使用图像特有的归纳偏置

处理方式就是把原先的图片分成一个一个的patch，符合transformer的序列长度输入，把这些patch的经过处理之后就可以看作一个一个单词，一般是把一个图片看作很多16*16的单词
首先是经过一个映射层，把每个patch进行处理，然后跟每个patch的位置编码进行一个结合，作为一个输入
这里还引入了一个特殊的embeding，即class token，因为每个token是相互学习的，所以用一个特定的token就可以学到所有的信息
至于为什么要用这个特殊的cls，作者也做了消融实验，也可以采用全局平均池化，得到一个全局的特征然后去做分类，其实效果都是差不多的，只是为了尽量去接近NLP任务中，防止说针对CV进行改进所带来的影响，而不是transformer本身比较优秀
之后进入transformer模块，其中包括L个transformer层，先做多头自注意力，然后再经过MLP，最后把cls token作为图像的整体特征输出
在拿到输出后，在后面接一个MLP通用分类头，最后用交叉熵函数去进行模型的训练

那么由于本文是比较具有开创性的论文，把transformer引入CV领域，所以会跟原先的CV中表现优秀的残差网络进行对比
从结果上来看，在中小规模的数据集上进行预训练之后，得到的结果会比传统的卷积神经网络会差一点，这是由于其缺少像卷积神经网络的归纳偏置
所谓的图像归纳偏置，其一是locality，就是相邻的区域其关联性是比较强的，其二是平移等变性，即先做平移还是先做卷积的结果都一样，具体来说，对于一个物体在图片中无论是在什么位置输出的结果都一样
有了这些先验信息，传统的卷积网络就可以从较少数据学到较好的模型
不过当ViT模型，如果在够大的数据集上进行预训练，学习到足够多的信息，就可以在数据学习上取得比卷积神经网络更好的结果







