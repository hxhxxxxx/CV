**摘要部分**
深的网络训练非常困难，如何解决该问题
介绍了自己的层深，且更低的运算复杂度，只把原来的CNN结构替换成了残差网络，就取得了很好的结果

第一张介绍图：
表示一个问题：对比的层数，训练更深的层数，会导致训练误差变大

**介绍部分**
深度神经网络的优势，可以有深度的层，不同程度的层会得到不同程度的feature，比如一些低级的视觉特征与高级语义的视觉特征
但是随着网络变深，会出现梯度爆炸或者消失，为了解决该问题，初始权重调整，以及中间过程中normalization
不过当加到一定程度之后，还是会出现精度变差，这并不是过拟合，训练误差与测试误差都会变大，如果是过拟合的话，应该是训练误差很好，但是测试误差变差

残差的大致思路就是：浅层网络得到的一个输出，会作为下一个网络的输入，但是这里不把它直接输入，而是用原先一个真实值剪去这个值，去训练这个差值，
然后输出的结果再加上浅层网络的输入作为一个输出，这样就可以保留原来的，训练效果不会降低
而且残差连接这里不需要训练

残差连接如何处理输入输出不同维度的问题
1.添加对应数量的0    2.类似于全连接的投影，用一个1*1的卷积层   3. 所有的连接都做投影
2.3效果接近，比1要好，而2的代价比3小，所以采取第2种方式

有无残差网络的对比
1.训练的速度更快，收敛更快
2.最后的效果更好
3.随着网络增加，有残差的效果更好

如果做到比较深的情况，通道数就会变大，计算复杂度会变大
所以就先做一个投影，降低一次维度，然后在降维的基础上做空间上的操作，然后再升维回去，这样复杂度就会跟原先相近
















